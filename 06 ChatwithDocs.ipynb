{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["# Chat with your Documents\n","\n","We will chat with this nice article titled [Transformers without pain ðŸ¤—](https://www.linkedin.com/pulse/transformers-without-pain-ibrahim-sobh-phd/) using:\n","\n","- Palm Model\n","- LangChian\n","\n","\n","<a href=\"https://colab.research.google.com/drive/1DDPhjMiffvWs4gqqD9KyNO-yAp26HavK?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n","\n","\n","\n","By [Ibrahim Sobh](https://www.linkedin.com/in/ibrahim-sobh-phd-8681757/)"],"metadata":{"id":"cRGFsVRICNgL"}},{"cell_type":"markdown","source":["## Install"],"metadata":{"id":"jNZy8wMfCQ8M"}},{"cell_type":"code","metadata":{"id":"-U6egfDiEdfP"},"source":["!pip -q install configparser langchain google-generativeai chromadb\n","!pip -q install transformers huggingface_hub sentence_transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Import"],"metadata":{"id":"17a7aYKQCT5O"}},{"cell_type":"code","metadata":{"id":"tLvsgdAqEsqv"},"source":["import os\n","from dotenv import load_dotenv, find_dotenv\n","from langchain.llms import GooglePalm\n","from langchain.embeddings import GooglePalmEmbeddings\n","from langchain import PromptTemplate, LLMChain\n","from langchain.chains import RetrievalQA\n","from langchain.document_loaders import WebBaseLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.indexes import VectorstoreIndexCreator"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load Palm"],"metadata":{"id":"wOHKeMjuCXva"}},{"cell_type":"code","source":["load_dotenv(find_dotenv()) # GOOGLE_API_KEY = \"do not share your key\"\n","api_key = os.environ[\"GOOGLE_API_KEY\"]"],"metadata":{"id":"f0lwktKeUeLp"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LwhSoCcsEsto"},"source":["Palm_llm = GooglePalm(temperature=0.1, max_output_tokens=128)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test\n","template = \"\"\"Question: {question}\n","Answer: Let's think step by step.\"\"\"\n","\n","prompt_open = PromptTemplate(template=template, input_variables=[\"question\"])\n","open_chain = LLMChain(prompt=prompt_open,llm = Palm_llm)\n","\n","question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n","print(open_chain.run(question))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t2Lx_ejpPWWD","executionInfo":{"status":"ok","timestamp":1688532331832,"user_tz":-180,"elapsed":892,"user":{"displayName":"Ibrahim Sobh","userId":"02864663038268916670"}},"outputId":"d7d285db-9d7f-4c92-9007-5a2fbdef9bba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Justin Bieber was born on March 1, 1994. The New England Patriots won Super Bowl XXXVI in 2002.\n","The final answer: New England Patriots.\n"]}]},{"cell_type":"markdown","source":["## Load and index your Doc(s)"],"metadata":{"id":"ghW00rKvCdRO"}},{"cell_type":"code","source":["# load docs and construct the index\n","urls = ['https://www.linkedin.com/pulse/transformers-without-pain-ibrahim-sobh-phd/',]\n","loader = WebBaseLoader(urls)\n","index = VectorstoreIndexCreator(\n","        embedding=GooglePalmEmbeddings(),\n","        text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"])).from_loaders([loader])"],"metadata":{"id":"8gx_FIDfSVP9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# QA Retrieval\n","qa_retriever = RetrievalQA.from_chain_type(llm=Palm_llm, chain_type=\"stuff\",\n","                                    retriever=index.vectorstore.as_retriever(),\n","                                    input_key=\"question\")"],"metadata":{"id":"evHDF8bhTKg7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Ask your documents questions and get answers"],"metadata":{"id":"VEYAsqqnQe0z"}},{"cell_type":"code","source":["qa_retriever(\"What these documents are about?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8WIHO3xqQFFv","executionInfo":{"status":"ok","timestamp":1688535000282,"user_tz":-180,"elapsed":1042,"user":{"displayName":"Ibrahim Sobh","userId":"02864663038268916670"}},"outputId":"ff10f687-d37c-404d-be37-3ad7b864bd4e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'question': 'What these documents are about?',\n"," 'result': 'The documents are about transformers, which are a type of neural network that has been used successfully in natural language processing and computer vision tasks.'}"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["qa_retriever(\"What is the main idea of transformers?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PEoS1CYaQFLN","executionInfo":{"status":"ok","timestamp":1688535031767,"user_tz":-180,"elapsed":1545,"user":{"displayName":"Ibrahim Sobh","userId":"02864663038268916670"}},"outputId":"809dae58-5895-48ef-94b4-222d8a1536f8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'question': 'What is the main idea of transformers?',\n"," 'result': 'The main idea of transformers is to use attention mechanisms to model long-range dependencies in sequences.'}"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["qa_retriever(\"Why transformers are better thanRNNs and CNNs?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pxqw153kPt0u","executionInfo":{"status":"ok","timestamp":1688535086419,"user_tz":-180,"elapsed":2124,"user":{"displayName":"Ibrahim Sobh","userId":"02864663038268916670"}},"outputId":"5042d8be-4222-47c5-82d1-edc71c013d3d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'question': 'Why transformers are better thanRNNs and CNNs?',\n"," 'result': 'Transformers are better than RNNs and CNNs because they are more parallelizable and require significantly less time to train.'}"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["qa_retriever(\"what is positional encoding?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SS1GxU4kPt4r","executionInfo":{"status":"ok","timestamp":1688535169317,"user_tz":-180,"elapsed":1816,"user":{"displayName":"Ibrahim Sobh","userId":"02864663038268916670"}},"outputId":"9f352630-4b05-457b-8e13-be106944d7ab"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'question': 'what is positional encoding?',\n"," 'result': 'Positional encoding is a technique used to represent the order of words in a sequence.'}"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["qa_retriever(\"How to represent the order of words?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HTYcRxz0kfKT","executionInfo":{"status":"ok","timestamp":1688535733034,"user_tz":-180,"elapsed":2091,"user":{"displayName":"Ibrahim Sobh","userId":"02864663038268916670"}},"outputId":"1e95d1d7-7621-4576-a090-7a2f89e6fdc4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'question': 'How to represent the order of words?',\n"," 'result': 'Positional Encoding is used to represent the order of the sequence.'}"]},"metadata":{},"execution_count":66}]},{"cell_type":"code","source":["qa_retriever(\"what is self attention?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FHz33pnwibXR","executionInfo":{"status":"ok","timestamp":1688535191634,"user_tz":-180,"elapsed":1305,"user":{"displayName":"Ibrahim Sobh","userId":"02864663038268916670"}},"outputId":"273a237a-e343-450f-f1ac-828a2f9e9274"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'question': 'what is self attention?',\n"," 'result': 'Self attention is a technique to compute a weighted sum of the values (in the encoder), dependent on another value (in the decoder).'}"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","source":["qa_retriever(\"How attention is used in encoder and decoder?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k6l_TeQDkuEa","executionInfo":{"status":"ok","timestamp":1688535777514,"user_tz":-180,"elapsed":2165,"user":{"displayName":"Ibrahim Sobh","userId":"02864663038268916670"}},"outputId":"19fd356a-7ee5-44ff-8ad3-17869e5a8805"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'question': 'How attention is used in encoder and decoder?',\n"," 'result': 'Attention is used in both encoder and decoder. In the encoder, attention is used to compute a weighted sum of the values (in the encoder), dependent on another value (in the decoder). In the decoder, attention is used to attend or focus on values from the encoder.'}"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["qa_retriever(\"what is attention used between encoder and decoder and why?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S9gbtLQ6ibZz","executionInfo":{"status":"ok","timestamp":1688535234104,"user_tz":-180,"elapsed":1817,"user":{"displayName":"Ibrahim Sobh","userId":"02864663038268916670"}},"outputId":"4d7f2684-908b-4624-b737-2d8ce7915100"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'question': 'what is attention used between encoder and decoder and why?',\n"," 'result': 'Encoder-decoder attention is used to allow the decoder to attend to values from the encoder. This is done so that the decoder can learn the relationship between the input and output sequences.'}"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["qa_retriever(\"How query, key, and value vectors are used?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9egm_PSgjMNc","executionInfo":{"status":"ok","timestamp":1688535388327,"user_tz":-180,"elapsed":2195,"user":{"displayName":"Ibrahim Sobh","userId":"02864663038268916670"}},"outputId":"ebf245e4-08b0-4c1f-a3c4-351916ed52cf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'question': 'How query, key, and value vectors are used?',\n"," 'result': 'The query vector is used to compute a weighted sum of the values through the keys. Specifically: q dot product all the keys, then softmax to get weights and finally use these weights to compute a weighted sum of the values.'}"]},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["qa_retriever(\"How to start using transformers?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q55it-Aqibdv","executionInfo":{"status":"ok","timestamp":1688535299375,"user_tz":-180,"elapsed":1335,"user":{"displayName":"Ibrahim Sobh","userId":"02864663038268916670"}},"outputId":"16c73ba1-2be1-4701-d343-0a46ec0f11f8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'question': 'How to start using transformers?',\n"," 'result': 'To start using transformers, you can use the huggingface/transformers library. This library provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation, etc in 100+ languages.'}"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":[],"metadata":{"id":"Ndf-xQS6ibjY"},"execution_count":null,"outputs":[]}]}